# ğŸ“ RAG ì‹œìŠ¤í…œ Agent í•™ìŠµ ë° í›ˆë ¨ ì „ëµ

## ğŸ“‹ ë¬¸ì„œ ê°œìš”

ë³¸ ë¬¸ì„œëŠ” KSys ì‚°ì—…ìš© ì„¼ì„œ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì˜ **RAG(Retrieval-Augmented Generation) ê¸°ë°˜ Multi-Agent ì‹œìŠ¤í…œ**ì˜ í•™ìŠµ ë° í›ˆë ¨ ì „ëµì„ ìƒì„¸íˆ ê¸°ìˆ í•©ë‹ˆë‹¤.

**ì‘ì„±ì¼**: 2025-09-01  
**ë²„ì „**: 1.0  
**ëŒ€ìƒ ì‹œìŠ¤í…œ**: reflex-ksys-refactor AI Engine

---

## ğŸ—ï¸ í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„

### ê¸°ì¡´ êµ¬ì¶•ëœ í•™ìŠµ ì¸í”„ë¼

ìš°ë¦¬ ì‹œìŠ¤í…œì€ ì´ë¯¸ **ê³ ë„ë¡œ ë°œì „ëœ í•™ìŠµ ê¸°ë°˜ êµ¬ì¡°**ë¥¼ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤:

**ğŸ“Š ë¶„ì„ ì—”ì§„ë“¤:**
- **PandasAnalysisEngine**: 711ì¤„ì˜ ê³ ê¸‰ ë°ì´í„° ë¶„ì„ (ML/í†µê³„)
- **RealDataAuditSystem**: 412ì¤„ì˜ ë°ì´í„° í’ˆì§ˆ ê°ì‚¬  
- **Multi-Agent System**: Research â†’ Analysis â†’ Review íŒŒì´í”„ë¼ì¸
- **VisualizationGenerator**: 320ì¤„ì˜ ë™ì  ì‹œê°í™” ìƒì„±
- **RAG Engine**: 497ì¤„ì˜ ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ ì¡°í•©

### í˜„ì¬ ë°ì´í„° ìì‚°

**ì‹¤ì‹œê°„ ì„¼ì„œ ë°ì´í„° (300ë§Œ+ ë ˆì½”ë“œ)**
```python
sensor_data_assets = {
    'influx_hist': 3_060_283,           # ì‹œê³„ì—´ ë°ì´í„°
    'influx_latest': 'real_time',       # ì‹¤ì‹œê°„ ë°ì´í„°  
    'qc_rules': 'quality_thresholds',   # í’ˆì§ˆ ê·œì¹™
    'features_5m': 'statistical_features',  # í†µê³„ì  íŠ¹ì„±
    'tech_ind_1m': 'technical_indicators'   # ê¸°ìˆ ì  ì§€í‘œ
}
```

**ë„ë©”ì¸ ì§€ì‹ë² ì´ìŠ¤**
- **17ê°œ ì „ë¬¸ ì§€ì‹ í•­ëª©**: ì„¼ì„œ ì‚¬ì–‘, ë¬¸ì œí•´ê²°, ìš´ì˜íŒ¨í„´, ìœ ì§€ë³´ìˆ˜, ìƒê´€ê´€ê³„
- **TF-IDF ë²¡í„°í™”**: ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ì§€ì›
- **ë©”íƒ€ë°ì´í„° êµ¬ì¡°í™”**: JSON í˜•íƒœì˜ êµ¬ì¡°í™”ëœ ì§€ì‹

---

## ğŸ¯ RAG Agent í•™ìŠµ ì „ëµ (5ë‹¨ê³„)

### 1ë‹¨ê³„: ë°ì´í„° ê¸°ë°˜ í•™ìŠµ (Data-Driven Learning)

#### ğŸ” ì‹¤ì‹œê°„ í”¼ë“œë°± í•™ìŠµ ì‹œìŠ¤í…œ

```python
class AgentLearningSystem:
    """ì‹¤ì‹œê°„ í”¼ë“œë°± ê¸°ë°˜ í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.performance_metrics = {
            'response_accuracy': [],      # ì‘ë‹µ ì •í™•ë„
            'user_satisfaction': [],      # ì‚¬ìš©ì ë§Œì¡±ë„
            'prediction_accuracy': [],    # ì˜ˆì¸¡ ì •í™•ë„
            'false_positive_rate': []     # ì˜¤íƒë¥ 
        }
        
        # í•™ìŠµ ì„¤ì •
        self.learning_rate = 0.1
        self.feedback_weight = 0.8
        self.system_metric_weight = 0.2
    
    async def learn_from_interaction(self, query, response, user_feedback):
        """ì‚¬ìš©ì ìƒí˜¸ì‘ìš©ì—ì„œ í•™ìŠµ"""
        
        # 1. ì‘ë‹µ í’ˆì§ˆ í‰ê°€
        quality_score = await self.evaluate_response_quality(query, response)
        
        # 2. ì‚¬ìš©ì ë§Œì¡±ë„ ìˆ˜ì§‘
        satisfaction = user_feedback.get('rating', 0)  # 1-5 ì ìˆ˜
        
        # 3. í•™ìŠµ ë°ì´í„° ì €ì¥
        learning_record = {
            'query': query,
            'response': response,
            'quality_score': quality_score,
            'user_satisfaction': satisfaction,
            'timestamp': datetime.now(),
            'context_used': response.get('context_data', {}),
            'agent_path': response.get('agent_workflow', 'unknown')
        }
        
        await self.store_learning_data(learning_record)
        
        # 4. ì§€ì‹ë² ì´ìŠ¤ ë™ì  ì—…ë°ì´íŠ¸
        if satisfaction >= 4:  # ì¢‹ì€ ì‘ë‹µ (4-5ì )
            await self.update_knowledge_base(query, response, 'positive')
            await self.reinforce_successful_patterns(learning_record)
        elif satisfaction <= 2:  # ë‚˜ìœ ì‘ë‹µ (1-2ì )
            await self.update_knowledge_base(query, response, 'negative')
            await self.analyze_failure_patterns(learning_record)
        
        # 5. ì‹¤ì‹œê°„ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì¡°ì •
        await self.adjust_model_weights(learning_record)
    
    async def evaluate_response_quality(self, query, response):
        """ì‘ë‹µ í’ˆì§ˆ ìë™ í‰ê°€"""
        
        quality_factors = {}
        
        # 1. ì‘ë‹µ ì™„ì„±ë„ (30%)
        completeness = len(response.split()) / max(50, len(query.split()) * 10)
        quality_factors['completeness'] = min(completeness, 1.0) * 0.3
        
        # 2. ê´€ë ¨ì„± ì ìˆ˜ (25%)
        relevance = await self.calculate_semantic_similarity(query, response)
        quality_factors['relevance'] = relevance * 0.25
        
        # 3. ê¸°ìˆ ì  ì •í™•ì„± (25%)
        technical_accuracy = await self.verify_technical_facts(response)
        quality_factors['technical_accuracy'] = technical_accuracy * 0.25
        
        # 4. ì‹¤í–‰ ê°€ëŠ¥ì„± (20%)
        actionability = await self.assess_actionability(response)
        quality_factors['actionability'] = actionability * 0.20
        
        return sum(quality_factors.values())
```

#### ğŸ“ˆ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì  ì‹œìŠ¤í…œ

```python
async def evaluate_agent_performance(self):
    """ì—ì´ì „íŠ¸ ì„±ëŠ¥ ì¢…í•© í‰ê°€"""
    
    # 1. ì˜ˆì¸¡ ì •í™•ë„ ë¶„ì„ (PandasAnalysisEngine í™œìš©)
    predictions_df = await self.collect_prediction_data()
    accuracy_metrics = await self.pandas_engine.analyze_sensor_data(
        sensors=['prediction_accuracy', 'response_time', 'user_satisfaction'],
        analysis_type="ml_evaluation",
        hours=168  # 1ì£¼ì¼
    )
    
    # 2. ì‘ë‹µ ì‹œê°„ íŠ¸ë Œë“œ ë¶„ì„
    response_times = await self.collect_response_times()
    performance_analysis = await self.pandas_engine.analyze_sensor_data(
        sensors=['response_time_ms'],
        analysis_type="performance_optimization", 
        hours=24
    )
    
    # 3. ë°ì´í„° í’ˆì§ˆ ê°ì‚¬ (RealDataAuditSystem í™œìš©)
    audit_result = await self.audit_system.analyze_sensor_data_gaps(
        sensor_name='ai_responses',
        analysis_hours=24
    )
    
    # 4. ì¢…í•© ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
    performance_score = {
        'accuracy_score': accuracy_metrics.confidence_score,
        'speed_score': self.calculate_speed_score(performance_analysis),
        'quality_score': audit_result.quality_score,
        'overall_score': 0.0
    }
    
    performance_score['overall_score'] = (
        performance_score['accuracy_score'] * 0.4 +
        performance_score['speed_score'] * 0.3 +
        performance_score['quality_score'] * 0.3
    )
    
    return performance_score
```

### 2ë‹¨ê³„: ê°•í™” í•™ìŠµ (Reinforcement Learning)

#### ğŸ¯ Q-Learning ê¸°ë°˜ ì •ì±… ìµœì í™”

```python
class ReinforcementLearningAgent:
    """ê°•í™”í•™ìŠµ ê¸°ë°˜ ì—ì´ì „íŠ¸ ì •ì±… ìµœì í™”"""
    
    def __init__(self):
        # Q-Table ì´ˆê¸°í™”
        self.q_table = {}  # (state, action) -> Qê°’
        
        # í•™ìŠµ íŒŒë¼ë¯¸í„°
        self.learning_rate = 0.1        # í•™ìŠµë¥ 
        self.discount_factor = 0.9      # í• ì¸ ì¸ìˆ˜
        self.epsilon = 0.1              # íƒí—˜ í™•ë¥ 
        self.epsilon_decay = 0.995      # íƒí—˜ í™•ë¥  ê°ì†Œìœ¨
        
        # ìƒíƒœ ë° í–‰ë™ ê³µê°„ ì •ì˜
        self.states = [
            'simple_query', 'complex_query', 'anomaly_detection',
            'trend_analysis', 'troubleshooting', 'maintenance_advice'
        ]
        
        self.actions = [
            'single_rag', 'research_analysis', 'full_multi_agent',
            'pandas_analysis', 'audit_system', 'visualization_only'
        ]
    
    async def select_action(self, state):
        """Îµ-greedy ì •ì±…ìœ¼ë¡œ í–‰ë™ ì„ íƒ"""
        
        if np.random.random() < self.epsilon:
            # íƒí—˜: ë¬´ì‘ìœ„ í–‰ë™
            return np.random.choice(self.actions)
        else:
            # í™œìš©: ìµœì  í–‰ë™
            q_values = [self.q_table.get((state, action), 0) for action in self.actions]
            best_action_idx = np.argmax(q_values)
            return self.actions[best_action_idx]
    
    async def update_policy(self, state, action, reward, next_state):
        """Q-Learning ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì •ì±… ì—…ë°ì´íŠ¸"""
        
        # í˜„ì¬ Qê°’
        current_q = self.q_table.get((state, action), 0)
        
        # ë‹¤ìŒ ìƒíƒœì˜ ìµœëŒ€ Qê°’
        next_q_values = [
            self.q_table.get((next_state, a), 0) 
            for a in self.actions
        ]
        next_max_q = max(next_q_values) if next_q_values else 0
        
        # Qê°’ ì—…ë°ì´íŠ¸ (Bellman Equation)
        new_q = current_q + self.learning_rate * (
            reward + self.discount_factor * next_max_q - current_q
        )
        
        self.q_table[(state, action)] = new_q
        
        # Îµ ê°ì†Œ (íƒí—˜ â†’ í™œìš©)
        self.epsilon = max(0.01, self.epsilon * self.epsilon_decay)
    
    def calculate_reward(self, query, response, user_feedback, system_metrics):
        """ë‹¤ì°¨ì› ë³´ìƒ í•¨ìˆ˜"""
        
        reward = 0.0
        
        # 1. ì‚¬ìš©ì ë§Œì¡±ë„ (40% ê°€ì¤‘ì¹˜)
        user_rating = user_feedback.get('rating', 0)  # 1-5
        normalized_rating = (user_rating - 1) / 4     # 0-1ë¡œ ì •ê·œí™”
        reward += normalized_rating * 0.4
        
        # 2. ì‹œìŠ¤í…œ ì •í™•ë„ (30% ê°€ì¤‘ì¹˜)
        accuracy = system_metrics.get('accuracy', 0)
        reward += accuracy * 0.3
        
        # 3. ì‘ë‹µ ì†ë„ (20% ê°€ì¤‘ì¹˜)
        response_time = system_metrics.get('response_time', 10)  # ì´ˆ
        speed_score = max(0, 1.0 - (response_time - 2.0) / 8.0)  # 2ì´ˆ ê¸°ì¤€
        reward += speed_score * 0.2
        
        # 4. QC ê·œì¹™ ì¤€ìˆ˜ (10% ê°€ì¤‘ì¹˜)
        qc_compliance = system_metrics.get('qc_compliance', 0)
        reward += qc_compliance * 0.1
        
        # 5. í˜ë„í‹° ì ìš©
        if user_feedback.get('error_reported', False):
            reward -= 0.5  # ì˜¤ë¥˜ ë³´ê³ ì‹œ í˜ë„í‹°
        
        if system_metrics.get('timeout', False):
            reward -= 0.3  # íƒ€ì„ì•„ì›ƒì‹œ í˜ë„í‹°
        
        return max(-1.0, min(1.0, reward))  # -1 ~ 1 ë²”ìœ„ë¡œ í´ë¨í•‘
```

### 3ë‹¨ê³„: ì§€ì‹ë² ì´ìŠ¤ ìë™ í™•ì¥ (Knowledge Base Expansion)

#### ğŸ§  ë™ì  ì§€ì‹ í•™ìŠµ ì‹œìŠ¤í…œ

```python
class KnowledgeExpansionSystem:
    """ì§€ì‹ë² ì´ìŠ¤ ìë™ í™•ì¥ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.knowledge_builder = KnowledgeBuilder()
        self.pattern_detector = PatternDetectionEngine()
        self.confidence_threshold = 0.8
        
    async def learn_from_sensor_patterns(self):
        """ì„¼ì„œ ë°ì´í„° íŒ¨í„´ì—ì„œ ìë™ í•™ìŠµ"""
        
        # 1. ì´ìƒ íŒ¨í„´ ìë™ ê°ì§€
        anomaly_patterns = await self.detect_recurring_anomalies()
        
        for pattern in anomaly_patterns:
            if pattern['confidence'] > self.confidence_threshold:
                new_knowledge = {
                    "content": f"{pattern['sensor']}ì—ì„œ {pattern['condition']} ì¡°ê±´ì¼ ë•Œ "
                              f"{pattern['outcome']} ê²°ê³¼ê°€ {pattern['frequency']:.1%} í™•ë¥ ë¡œ ë°œìƒí•©ë‹ˆë‹¤. "
                              f"ê¶Œì¥ ì¡°ì¹˜: {pattern['recommended_action']}",
                    "content_type": "learned_pattern",
                    "metadata": {
                        "sensor_tag": pattern['sensor'],
                        "pattern_type": pattern['type'],
                        "confidence": pattern['confidence'],
                        "learned_from": "system_observation",
                        "occurrence_frequency": pattern['frequency'],
                        "last_observed": pattern['last_seen'],
                        "sample_size": pattern['sample_count']
                    }
                }
                await self.knowledge_builder.add_knowledge(new_knowledge)
                print(f"âœ… ìƒˆë¡œìš´ íŒ¨í„´ í•™ìŠµ: {pattern['sensor']} - {pattern['type']}")
    
    async def detect_recurring_anomalies(self):
        """ë°˜ë³µë˜ëŠ” ì´ìƒ íŒ¨í„´ ê°ì§€"""
        
        # ìµœê·¼ 30ì¼ê°„ ë°ì´í„° ë¶„ì„
        sql = """
        SELECT 
            tag_name,
            DATE_TRUNC('hour', ts) as hour_bucket,
            AVG(value) as avg_value,
            STDDEV(value) as std_value,
            COUNT(*) as sample_count
        FROM public.influx_hist 
        WHERE ts >= NOW() - INTERVAL '30 days'
        GROUP BY tag_name, hour_bucket
        HAVING COUNT(*) >= 10
        ORDER BY tag_name, hour_bucket
        """
        
        data = await q(sql, ())
        patterns = []
        
        # ì„¼ì„œë³„ íŒ¨í„´ ë¶„ì„
        for sensor_group in self.group_by_sensor(data):
            sensor_name = sensor_group['sensor']
            values = sensor_group['values']
            
            # í†µê³„ì  ì´ìƒì¹˜ ê°ì§€
            mean_val = np.mean(values)
            std_val = np.std(values)
            
            for i, value in enumerate(values):
                z_score = abs((value - mean_val) / std_val) if std_val > 0 else 0
                
                if z_score > 2.0:  # 2 í‘œì¤€í¸ì°¨ ì´ìƒ
                    pattern = {
                        'sensor': sensor_name,
                        'type': 'statistical_anomaly',
                        'condition': f'ê°’ì´ {mean_val:.2f} Â± {2*std_val:.2f} ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨',
                        'outcome': 'anomaly_detected',
                        'frequency': len([v for v in values if abs((v - mean_val) / std_val) > 2.0]) / len(values),
                        'confidence': min(0.95, z_score / 3.0),
                        'recommended_action': f'{sensor_name} ì„¼ì„œ ì ê²€ ë° ìº˜ë¦¬ë¸Œë ˆì´ì…˜',
                        'last_seen': datetime.now(),
                        'sample_count': len(values)
                    }
                    patterns.append(pattern)
        
        return patterns
    
    async def learn_from_successful_responses(self):
        """ê³ í’ˆì§ˆ ì‘ë‹µì—ì„œ ì§€ì‹ ì¶”ì¶œ"""
        
        # ë†’ì€ í‰ê°€ë¥¼ ë°›ì€ ì‘ë‹µë“¤ ì¡°íšŒ
        sql = """
        SELECT query, response, user_rating, system_metrics, timestamp
        FROM ai_learning_data 
        WHERE user_rating >= 4 
        AND timestamp >= NOW() - INTERVAL '7 days'
        ORDER BY user_rating DESC, timestamp DESC
        LIMIT 50
        """
        
        successful_interactions = await q(sql, ())
        
        for interaction in successful_interactions:
            # ìì—°ì–´ ì²˜ë¦¬ë¡œ í•µì‹¬ ì§€ì‹ ì¶”ì¶œ
            extracted_knowledge = await self.extract_knowledge_from_response(
                interaction['query'], 
                interaction['response']
            )
            
            if extracted_knowledge and extracted_knowledge['confidence'] > 0.7:
                await self.knowledge_builder.add_knowledge(extracted_knowledge)
                print(f"âœ… ì„±ê³µ ì‘ë‹µì—ì„œ ì§€ì‹ ì¶”ì¶œ: {extracted_knowledge['content'][:50]}...")
    
    async def extract_knowledge_from_response(self, query, response):
        """ì‘ë‹µì—ì„œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì§€ì‹ ì¶”ì¶œ"""
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì§€ì‹ ì¶”ì¶œ
        knowledge_patterns = {
            'sensor_relationship': r'(\w+)ê³¼ (\w+) ì„¼ì„œ.*?ê´€ë ¨|ì—°ê´€|ìƒê´€',
            'threshold_info': r'(\w+) ì„¼ì„œ.*?(\d+\.?\d*)\s*(ì´ìƒ|ì´í•˜|ì´ˆê³¼|ë¯¸ë§Œ)',
            'maintenance_action': r'(\w+) ì„¼ì„œ.*?(ì ê²€|êµì²´|ìº˜ë¦¬ë¸Œë ˆì´ì…˜|ì²­ì†Œ).*?í•„ìš”',
            'operational_pattern': r'(\w+) ì‹œê°„.*?(\w+) ì„¼ì„œ.*?(\d+\.?\d*).*?(ì¦ê°€|ê°ì†Œ|ë³€í™”)'
        }
        
        extracted_facts = []
        
        for pattern_type, regex_pattern in knowledge_patterns.items():
            matches = re.findall(regex_pattern, response, re.IGNORECASE)
            
            for match in matches:
                fact = {
                    "content": f"í•™ìŠµëœ ì‚¬ì‹¤: {' '.join(match)}",
                    "content_type": f"learned_{pattern_type}",
                    "metadata": {
                        "extracted_from": "successful_response",
                        "original_query": query[:100],
                        "confidence": 0.8,
                        "extraction_method": "regex_pattern",
                        "pattern_type": pattern_type
                    }
                }
                extracted_facts.append(fact)
        
        # ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ì§€ì‹ ë°˜í™˜
        if extracted_facts:
            return max(extracted_facts, key=lambda x: x['metadata']['confidence'])
        
        return None
```

### 4ë‹¨ê³„: Multi-Agent í˜‘ì—… í•™ìŠµ (Collaborative Learning)

#### ğŸ¤ ì—ì´ì „íŠ¸ ê°„ ì§€ì‹ ê³µìœ  ë° ìµœì í™”

```python
class CollaborativeLearningSystem:
    """Multi-Agent í˜‘ì—… í•™ìŠµ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.agent_performance = {
            'research_agent': {
                'accuracy': [], 'speed': [], 'data_coverage': []
            },
            'analysis_agent': {
                'accuracy': [], 'speed': [], 'insight_quality': []
            },
            'review_agent': {
                'accuracy': [], 'speed': [], 'error_detection': []
            }
        }
        
        self.collaboration_patterns = {}
        self.optimal_workflows = {}
    
    async def optimize_agent_collaboration(self):
        """ì—ì´ì „íŠ¸ í˜‘ì—… íŒ¨í„´ ìµœì í™”"""
        
        print("ğŸ¤ Multi-Agent í˜‘ì—… ìµœì í™” ì‹œì‘...")
        
        # 1. ê° ì—ì´ì „íŠ¸ ê°œë³„ ì„±ëŠ¥ ë¶„ì„
        for agent_name in self.agent_performance:
            performance = await self.analyze_individual_agent_performance(agent_name)
            
            print(f"ğŸ“Š {agent_name} ì„±ëŠ¥: {performance}")
            
            # ì„±ëŠ¥ì´ ê¸°ì¤€ ì´í•˜ì¸ ì—ì´ì „íŠ¸ ì¬í›ˆë ¨
            if performance['overall_score'] < 0.8:
                await self.retrain_underperforming_agent(agent_name, performance)
        
        # 2. ì—ì´ì „íŠ¸ ê°„ ì§€ì‹ ì „ì´ í•™ìŠµ
        await self.transfer_knowledge_between_agents()
        
        # 3. ìµœì  í˜‘ì—… íŒ¨í„´ í•™ìŠµ
        await self.learn_optimal_collaboration_patterns()
        
        # 4. ë™ì  ì›Œí¬í”Œë¡œìš° ì¡°ì •
        await self.optimize_workflow_routing()
    
    async def analyze_individual_agent_performance(self, agent_name):
        """ê°œë³„ ì—ì´ì „íŠ¸ ì„±ëŠ¥ ìƒì„¸ ë¶„ì„"""
        
        # ìµœê·¼ 7ì¼ê°„ ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘
        sql = f"""
        SELECT 
            execution_time,
            accuracy_score,
            user_satisfaction,
            error_count,
            success_rate
        FROM agent_performance_log 
        WHERE agent_name = %s 
        AND timestamp >= NOW() - INTERVAL '7 days'
        ORDER BY timestamp DESC
        """
        
        performance_data = await q(sql, (agent_name,))
        
        if not performance_data:
            return {'overall_score': 0.5, 'status': 'insufficient_data'}
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        avg_execution_time = np.mean([d['execution_time'] for d in performance_data])
        avg_accuracy = np.mean([d['accuracy_score'] for d in performance_data])
        avg_satisfaction = np.mean([d['user_satisfaction'] for d in performance_data])
        error_rate = np.mean([d['error_count'] for d in performance_data])
        success_rate = np.mean([d['success_rate'] for d in performance_data])
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        speed_score = max(0, 1.0 - (avg_execution_time - 1.0) / 4.0)  # 1ì´ˆ ê¸°ì¤€
        quality_score = (avg_accuracy + avg_satisfaction) / 2
        reliability_score = success_rate * (1 - error_rate)
        
        overall_score = (
            speed_score * 0.3 + 
            quality_score * 0.4 + 
            reliability_score * 0.3
        )
        
        return {
            'overall_score': overall_score,
            'speed_score': speed_score,
            'quality_score': quality_score,
            'reliability_score': reliability_score,
            'avg_execution_time': avg_execution_time,
            'avg_accuracy': avg_accuracy,
            'error_rate': error_rate,
            'status': 'analyzed'
        }
    
    async def adaptive_agent_selection(self, query_analysis):
        """ì¿¼ë¦¬ íŠ¹ì„±ì— ë”°ë¥¸ ì ì‘ì  ì—ì´ì „íŠ¸ ì„ íƒ"""
        
        complexity = query_analysis.get('complexity', 0.5)
        domain = query_analysis.get('domain', 'general')
        urgency = query_analysis.get('urgency', 'normal')
        
        # ë³µì¡ë„ë³„ ì „ëµ ì„ íƒ
        if complexity < 0.3:
            # ë‹¨ìˆœí•œ ì¿¼ë¦¬: RAGë§Œ ì‚¬ìš©
            workflow = "single_rag"
            agents = ["rag_engine"]
            
        elif complexity < 0.7:
            # ì¤‘ê°„ ë³µì¡ë„: Research + Analysis
            workflow = "research_analysis"
            agents = ["research_agent", "analysis_agent"]
            
        else:
            # ë³µì¡í•œ ì¿¼ë¦¬: Full Multi-Agent
            workflow = "full_multi_agent"
            agents = ["research_agent", "analysis_agent", "review_agent"]
        
        # ë„ë©”ì¸ë³„ íŠ¹í™” ì—ì´ì „íŠ¸ ì¶”ê°€
        if domain == "anomaly_detection":
            agents.append("pandas_analysis_engine")
        elif domain == "data_quality":
            agents.append("audit_system")
        elif domain == "visualization":
            agents.append("visualization_generator")
        
        # ê¸´ê¸‰ë„ë³„ ìš°ì„ ìˆœìœ„ ì¡°ì •
        if urgency == "high":
            # ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´ ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”
            execution_mode = "parallel"
        else:
            # í’ˆì§ˆ ì¤‘ì‹œ ìˆœì°¨ ì²˜ë¦¬
            execution_mode = "sequential"
        
        return {
            'workflow': workflow,
            'agents': agents,
            'execution_mode': execution_mode,
            'estimated_time': self.estimate_execution_time(agents, execution_mode),
            'confidence': self.calculate_workflow_confidence(workflow, query_analysis)
        }
    
    async def transfer_knowledge_between_agents(self):
        """ì—ì´ì „íŠ¸ ê°„ ì§€ì‹ ì „ì´"""
        
        # 1. Research Agentì˜ ì„±ê³µì ì¸ ê²€ìƒ‰ íŒ¨í„´ì„ Analysis Agentì— ì „ë‹¬
        research_patterns = await self.extract_successful_patterns('research_agent')
        await self.apply_patterns_to_agent('analysis_agent', research_patterns)
        
        # 2. Analysis Agentì˜ ì¸ì‚¬ì´íŠ¸ ìƒì„± íŒ¨í„´ì„ Review Agentì— ì „ë‹¬
        analysis_patterns = await self.extract_successful_patterns('analysis_agent')
        await self.apply_patterns_to_agent('review_agent', analysis_patterns)
        
        # 3. Review Agentì˜ í’ˆì§ˆ ê¸°ì¤€ì„ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë“¤ì— ì „íŒŒ
        quality_standards = await self.extract_quality_standards('review_agent')
        for agent in ['research_agent', 'analysis_agent']:
            await self.apply_quality_standards(agent, quality_standards)
        
        print("âœ… ì—ì´ì „íŠ¸ ê°„ ì§€ì‹ ì „ì´ ì™„ë£Œ")
```

### 5ë‹¨ê³„: ì—°ì† í•™ìŠµ (Continuous Learning)

#### ğŸ”„ ì‹¤ì‹œê°„ ì ì‘ í•™ìŠµ ì‹œìŠ¤í…œ

```python
class ContinuousLearningSystem:
    """ì—°ì† í•™ìŠµ ë° ì‹¤ì‹œê°„ ì ì‘ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.learning_scheduler = None
        self.learning_rate = 0.1
        self.adaptation_threshold = 0.05
        self.performance_history = []
        
    async def start_continuous_learning(self):
        """ì—°ì† í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì‹œì‘"""
        
        print("ğŸ”„ ì—°ì† í•™ìŠµ ì‹œìŠ¤í…œ ì‹œì‘...")
        self.learning_scheduler = asyncio.create_task(self.continuous_learning_loop())
        
    async def continuous_learning_loop(self):
        """ì§€ì†ì  í•™ìŠµ ë©”ì¸ ë£¨í”„"""
        
        while True:
            try:
                current_time = datetime.now()
                
                # 1. ì‹¤ì‹œê°„ í”¼ë“œë°± ì²˜ë¦¬ (ë§¤ 10ë¶„)
                if current_time.minute % 10 == 0:
                    await self.process_realtime_feedback()
                
                # 2. ì„±ëŠ¥ í‰ê°€ ë° ì¡°ì • (ë§¤ ì‹œê°„)
                if current_time.minute == 0:
                    await self.hourly_performance_evaluation()
                
                # 3. ì¼ì¼ ì§€ì‹ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ (ë§¤ì¼ 02:00)
                if current_time.hour == 2 and current_time.minute == 0:
                    await self.daily_knowledge_update()
                
                # 4. ì£¼ê°„ ëª¨ë¸ ìµœì í™” (ë§¤ì£¼ ì¼ìš”ì¼ 03:00)
                if current_time.weekday() == 6 and current_time.hour == 3:
                    await self.weekly_model_optimization()
                
                # 5. ì›”ê°„ ì „ë©´ ì¬í›ˆë ¨ (ë§¤ì›” 1ì¼ 04:00)
                if current_time.day == 1 and current_time.hour == 4:
                    await self.monthly_comprehensive_retraining()
                
                # 1ë¶„ ëŒ€ê¸°
                await asyncio.sleep(60)
                
            except Exception as e:
                print(f"âŒ ì—°ì† í•™ìŠµ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(300)  # 5ë¶„ í›„ ì¬ì‹œë„
    
    async def adaptive_learning_rate_adjustment(self):
        """ì„±ëŠ¥ì— ë”°ë¥¸ ì ì‘ì  í•™ìŠµë¥  ì¡°ì •"""
        
        recent_performance = await self.get_recent_performance_trend()
        
        if len(recent_performance) < 5:
            return  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¡°ì •í•˜ì§€ ì•ŠìŒ
        
        # ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„
        performance_trend = np.polyfit(range(len(recent_performance)), recent_performance, 1)[0]
        current_performance = recent_performance[-1]
        
        # í•™ìŠµë¥  ì¡°ì • ë¡œì§
        if current_performance > 0.9 and performance_trend > 0:
            # ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ê³  ìƒìŠ¹ ì¤‘ â†’ í•™ìŠµë¥  ê°ì†Œ (ì•ˆì •ì„± ì¤‘ì‹œ)
            self.learning_rate *= 0.95
            adjustment_reason = "high_performance_stable"
            
        elif current_performance < 0.7 or performance_trend < -0.05:
            # ì„±ëŠ¥ì´ ì €ì¡°í•˜ê±°ë‚˜ í•˜ë½ ì¤‘ â†’ í•™ìŠµë¥  ì¦ê°€ (ë¹ ë¥¸ ê°œì„ )
            self.learning_rate *= 1.1
            adjustment_reason = "low_performance_recovery"
            
        elif abs(performance_trend) < 0.01:
            # ì„±ëŠ¥ì´ ì •ì²´ ì¤‘ â†’ í•™ìŠµë¥  ì†Œí­ ì¦ê°€ (íƒí—˜ ì¦ê°€)
            self.learning_rate *= 1.05
            adjustment_reason = "performance_plateau"
            
        # í•™ìŠµë¥  ë²”ìœ„ ì œí•œ
        self.learning_rate = max(0.01, min(self.learning_rate, 0.5))
        
        print(f"ğŸ“Š í•™ìŠµë¥  ì¡°ì •: {self.learning_rate:.4f} (ì´ìœ : {adjustment_reason})")
    
    async def process_realtime_feedback(self):
        """ì‹¤ì‹œê°„ í”¼ë“œë°± ì²˜ë¦¬"""
        
        # ìµœê·¼ 10ë¶„ê°„ í”¼ë“œë°± ìˆ˜ì§‘
        sql = """
        SELECT query, response, user_rating, response_time, accuracy_score
        FROM ai_interactions 
        WHERE timestamp >= NOW() - INTERVAL '10 minutes'
        AND user_rating IS NOT NULL
        ORDER BY timestamp DESC
        """
        
        recent_feedback = await q(sql, ())
        
        if not recent_feedback:
            return
        
        # í”¼ë“œë°± ë¶„ì„ ë° ì¦‰ì‹œ ì ìš©
        for interaction in recent_feedback:
            # ë¶€ì •ì  í”¼ë“œë°±ì— ëŒ€í•œ ì¦‰ì‹œ í•™ìŠµ
            if interaction['user_rating'] <= 2:
                await self.immediate_correction_learning(interaction)
            
            # ê¸ì •ì  í”¼ë“œë°± íŒ¨í„´ ê°•í™”
            elif interaction['user_rating'] >= 4:
                await self.reinforce_positive_patterns(interaction)
        
        print(f"ğŸ”„ ì‹¤ì‹œê°„ í”¼ë“œë°± ì²˜ë¦¬ ì™„ë£Œ: {len(recent_feedback)}ê°œ ìƒí˜¸ì‘ìš©")
    
    async def weekly_model_optimization(self):
        """ì£¼ê°„ ëª¨ë¸ ìµœì í™”"""
        
        print("ğŸ“ˆ ì£¼ê°„ ëª¨ë¸ ìµœì í™” ì‹œì‘...")
        
        # 1. ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„
        weekly_performance = await self.collect_weekly_performance_data()
        
        # 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íŠœë‹
        optimal_params = await self.auto_tune_hyperparameters(weekly_performance)
        
        # 3. ëª¨ë¸ ê°€ì¤‘ì¹˜ ìµœì í™”
        await self.optimize_model_weights(weekly_performance)
        
        # 4. A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°˜ì˜
        ab_test_results = await self.analyze_ab_test_results()
        await self.apply_ab_test_winners(ab_test_results)
        
        # 5. ì„±ëŠ¥ ê°œì„  ë³´ê³ ì„œ ìƒì„±
        improvement_report = await self.generate_improvement_report(weekly_performance)
        
        print(f"âœ… ì£¼ê°„ ìµœì í™” ì™„ë£Œ: {improvement_report['improvement_percentage']:.1%} ì„±ëŠ¥ í–¥ìƒ")
    
    async def auto_tune_hyperparameters(self, performance_data):
        """ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
        
        # ê·¸ë¦¬ë“œ ì„œì¹˜ë¥¼ í†µí•œ ìµœì  íŒŒë¼ë¯¸í„° íƒìƒ‰
        param_grid = {
            'learning_rate': [0.05, 0.1, 0.15, 0.2],
            'correlation_threshold': [0.2, 0.3, 0.4, 0.5],
            'anomaly_threshold': [1.5, 2.0, 2.5, 3.0],
            'response_timeout': [5, 8, 10, 12]
        }
        
        best_params = {}
        best_score = 0
        
        for lr in param_grid['learning_rate']:
            for ct in param_grid['correlation_threshold']:
                for at in param_grid['anomaly_threshold']:
                    for rt in param_grid['response_timeout']:
                        
                        # íŒŒë¼ë¯¸í„° ì¡°í•© í…ŒìŠ¤íŠ¸
                        test_params = {
                            'learning_rate': lr,
                            'correlation_threshold': ct,
                            'anomaly_threshold': at,
                            'response_timeout': rt
                        }
                        
                        score = await self.evaluate_parameter_combination(
                            test_params, performance_data
                        )
                        
                        if score > best_score:
                            best_score = score
                            best_params = test_params
        
        # ìµœì  íŒŒë¼ë¯¸í„° ì ìš©
        await self.apply_optimal_parameters(best_params)
        
        return best_params
```

---

## ğŸ“Š í•™ìŠµ ì„±ê³¼ ì¸¡ì • ì§€í‘œ

### í•µì‹¬ KPI ì •ì˜

```python
learning_kpis = {
    # ì •í™•ë„ ì§€í‘œ
    'response_accuracy': {
        'target': 0.90,
        'current': 0.85,
        'measurement': 'user_rating >= 4 / total_responses',
        'weight': 0.25
    },
    
    'prediction_accuracy': {
        'target': 0.85,
        'current': 0.78,
        'measurement': 'correct_predictions / total_predictions',
        'weight': 0.20
    },
    
    'anomaly_detection_rate': {
        'target': 0.95,
        'current': 0.92,
        'measurement': 'detected_anomalies / actual_anomalies',
        'weight': 0.15
    },
    
    # íš¨ìœ¨ì„± ì§€í‘œ
    'average_response_time': {
        'target': 2.0,  # ì´ˆ
        'current': 2.3,
        'measurement': 'sum(response_times) / count(responses)',
        'weight': 0.15
    },
    
    'knowledge_coverage': {
        'target': 0.80,
        'current': 0.76,
        'measurement': 'answered_queries / total_queries',
        'weight': 0.10
    },
    
    'user_satisfaction': {
        'target': 4.0,  # 5ì  ë§Œì 
        'current': 4.2,
        'measurement': 'avg(user_ratings)',
        'weight': 0.15
    }
}
```

### ì„±ê³¼ ì¸¡ì • ëŒ€ì‹œë³´ë“œ

```python
async def generate_learning_performance_dashboard():
    """í•™ìŠµ ì„±ê³¼ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
    
    dashboard_data = {
        'overall_score': 0.0,
        'kpi_scores': {},
        'trend_analysis': {},
        'improvement_recommendations': []
    }
    
    # 1. ê° KPI ì ìˆ˜ ê³„ì‚°
    total_weighted_score = 0
    total_weight = 0
    
    for kpi_name, kpi_config in learning_kpis.items():
        current_value = await measure_kpi(kpi_name)
        target_value = kpi_config['target']
        weight = kpi_config['weight']
        
        # ì •ê·œí™”ëœ ì ìˆ˜ ê³„ì‚° (0-1)
        if kpi_name == 'average_response_time':
            # ì‘ë‹µì‹œê°„ì€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
            normalized_score = max(0, min(1, (target_value / current_value)))
        else:
            # ë‚˜ë¨¸ì§€ëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
            normalized_score = min(1, current_value / target_value)
        
        dashboard_data['kpi_scores'][kpi_name] = {
            'current': current_value,
            'target': target_value,
            'score': normalized_score,
            'status': 'good' if normalized_score >= 0.9 else 'warning' if normalized_score >= 0.7 else 'critical'
        }
        
        total_weighted_score += normalized_score * weight
        total_weight += weight
    
    # 2. ì „ì²´ ì ìˆ˜ ê³„ì‚°
    dashboard_data['overall_score'] = total_weighted_score / total_weight
    
    # 3. íŠ¸ë Œë“œ ë¶„ì„
    for kpi_name in learning_kpis.keys():
        trend = await analyze_kpi_trend(kpi_name, days=30)
        dashboard_data['trend_analysis'][kpi_name] = trend
    
    # 4. ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±
    dashboard_data['improvement_recommendations'] = await generate_improvement_recommendations(
        dashboard_data['kpi_scores']
    )
    
    return dashboard_data
```

---

## ğŸš€ êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: ê¸°ë°˜ êµ¬ì¶• (1-2ì£¼)

**Week 1: ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ**
- [ ] ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ UI êµ¬í˜„
- [ ] í•™ìŠµ ë°ì´í„° ì €ì¥ ìŠ¤í‚¤ë§ˆ ì„¤ê³„
- [ ] ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì  ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ê¸°ë³¸ ëŒ€ì‹œë³´ë“œ ê°œë°œ

**Week 2: í‰ê°€ ì‹œìŠ¤í…œ**
- [ ] ì‘ë‹µ í’ˆì§ˆ ìë™ í‰ê°€ ì•Œê³ ë¦¬ì¦˜
- [ ] KPI ì¸¡ì • ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ êµ¬ì¶•
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ë„êµ¬

### Phase 2: í•µì‹¬ í•™ìŠµ (3-4ì£¼)

**Week 3: ê°•í™”í•™ìŠµ êµ¬í˜„**
- [ ] Q-Learning ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
- [ ] ë³´ìƒ í•¨ìˆ˜ ì„¤ê³„ ë° íŠœë‹
- [ ] ìƒíƒœ-í–‰ë™ ê³µê°„ ì •ì˜
- [ ] ì •ì±… ìµœì í™” ì‹œìŠ¤í…œ

**Week 4: ì§€ì‹ë² ì´ìŠ¤ í™•ì¥**
- [ ] íŒ¨í„´ ìë™ ê°ì§€ ì‹œìŠ¤í…œ
- [ ] ë™ì  ì§€ì‹ ì¶”ê°€ ë©”ì»¤ë‹ˆì¦˜
- [ ] ì§€ì‹ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ
- [ ] ì¤‘ë³µ ì§€ì‹ ì œê±° ì•Œê³ ë¦¬ì¦˜

### Phase 3: ê³ ë„í™” (5-6ì£¼)

**Week 5: Multi-Agent ìµœì í™”**
- [ ] ì—ì´ì „íŠ¸ ê°„ ì§€ì‹ ì „ì´ ì‹œìŠ¤í…œ
- [ ] í˜‘ì—… íŒ¨í„´ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜
- [ ] ì ì‘ì  ì›Œí¬í”Œë¡œìš° ì„ íƒ
- [ ] ì„±ëŠ¥ ê¸°ë°˜ ì—ì´ì „íŠ¸ ì¡°ì •

**Week 6: ì—°ì† í•™ìŠµ ì‹œìŠ¤í…œ**
- [ ] ì‹¤ì‹œê°„ í•™ìŠµ ë£¨í”„ êµ¬í˜„
- [ ] ì ì‘ì  í•™ìŠµë¥  ì¡°ì •
- [ ] ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### Phase 4: ìµœì í™” ë° ë°°í¬ (7-8ì£¼)

**Week 7: ì‹œìŠ¤í…œ ìµœì í™”**
- [ ] í•™ìŠµ ì†ë„ ìµœì í™”
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
- [ ] ë³‘ë ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ìºì‹± ì „ëµ ìµœì í™”

**Week 8: í”„ë¡œë•ì…˜ ë°°í¬**
- [ ] ë¬´ì¤‘ë‹¨ í•™ìŠµ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ëª¨ë¸ ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ
- [ ] ë¡¤ë°± ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„
- [ ] ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ

---

## ğŸ’¡ ì‹¤ì „ ì ìš© ê°€ì´ë“œ

### ì¦‰ì‹œ ì‹œì‘ ê°€ëŠ¥í•œ í•™ìŠµ ë°©ë²•

#### 1. ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘
```html
<!-- AI ì‘ë‹µ í•˜ë‹¨ì— ì¶”ê°€ -->
<div class="feedback-section">
    <p>ì´ ì‘ë‹µì´ ë„ì›€ì´ ë˜ì—ˆë‚˜ìš”?</p>
    <button onclick="submitFeedback(5)">ğŸ‘ ë§¤ìš° ì¢‹ìŒ</button>
    <button onclick="submitFeedback(4)">ğŸ‘ ì¢‹ìŒ</button>
    <button onclick="submitFeedback(3)">ğŸ˜ ë³´í†µ</button>
    <button onclick="submitFeedback(2)">ğŸ‘ ì•„ì‰¬ì›€</button>
    <button onclick="submitFeedback(1)">ğŸ‘ ë§¤ìš° ì•„ì‰¬ì›€</button>
</div>
```

#### 2. í•™ìŠµ ë°ì´í„° ìŠ¤í‚¤ë§ˆ
```sql
CREATE TABLE ai_learning_data (
    id SERIAL PRIMARY KEY,
    query TEXT NOT NULL,
    response TEXT NOT NULL,
    user_rating INTEGER CHECK (user_rating BETWEEN 1 AND 5),
    response_time FLOAT,
    accuracy_score FLOAT,
    agent_workflow VARCHAR(50),
    context_data JSONB,
    timestamp TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_learning_data_rating ON ai_learning_data(user_rating);
CREATE INDEX idx_learning_data_timestamp ON ai_learning_data(timestamp);
```

#### 3. ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ êµ¬í˜„
```python
async def create_learning_dashboard():
    """ì‹¤ì‹œê°„ í•™ìŠµ ì„±ê³¼ ëŒ€ì‹œë³´ë“œ"""
    
    # ìµœê·¼ 24ì‹œê°„ ì„±ê³¼ ë°ì´í„°
    daily_stats = await q("""
        SELECT 
            AVG(user_rating) as avg_rating,
            AVG(response_time) as avg_response_time,
            COUNT(*) as total_interactions,
            COUNT(CASE WHEN user_rating >= 4 THEN 1 END) as positive_feedback
        FROM ai_learning_data 
        WHERE timestamp >= NOW() - INTERVAL '24 hours'
    """, ())
    
    # ì£¼ê°„ íŠ¸ë Œë“œ ë¶„ì„
    weekly_trend = await q("""
        SELECT 
            DATE(timestamp) as date,
            AVG(user_rating) as daily_rating,
            COUNT(*) as daily_count
        FROM ai_learning_data 
        WHERE timestamp >= NOW() - INTERVAL '7 days'
        GROUP BY DATE(timestamp)
        ORDER BY date
    """, ())
    
    return {
        'daily_performance': daily_stats[0] if daily_stats else {},
        'weekly_trend': weekly_trend,
        'improvement_rate': calculate_improvement_rate(weekly_trend)
    }
```

### ìš°ì„ ìˆœìœ„ êµ¬í˜„ í•­ëª©

**ğŸ¥‡ ë†’ì€ ìš°ì„ ìˆœìœ„ (ì¦‰ì‹œ êµ¬í˜„)**
1. ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ì‹œìŠ¤í…œ
2. ê¸°ë³¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì 
3. ì‘ë‹µ í’ˆì§ˆ ìë™ í‰ê°€
4. í•™ìŠµ ë°ì´í„° ì €ì¥ì†Œ êµ¬ì¶•

**ğŸ¥ˆ ì¤‘ê°„ ìš°ì„ ìˆœìœ„ (2-3ì£¼ ë‚´)**
1. ê°•í™”í•™ìŠµ ê¸°ë°˜ ì •ì±… ìµœì í™”
2. ì§€ì‹ë² ì´ìŠ¤ ìë™ í™•ì¥
3. Multi-Agent í˜‘ì—… ìµœì í™”
4. A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬

**ğŸ¥‰ ë‚®ì€ ìš°ì„ ìˆœìœ„ (ì¥ê¸° ê³„íš)**
1. ê³ ê¸‰ ìì—°ì–´ ì²˜ë¦¬ í†µí•©
2. ì™¸ë¶€ ì§€ì‹ ì†ŒìŠ¤ ì—°ë™
3. ë‹¤êµ­ì–´ í•™ìŠµ ì§€ì›
4. ê³ ê¸‰ ì‹œê°í™” ë° ë¶„ì„ ë„êµ¬

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê¸°ìˆ  ë¬¸ì„œ
- [AI_BUSINESS_LOGIC_DOCUMENTATION.md](./AI_BUSINESS_LOGIC_DOCUMENTATION.md)
- [AI_SYSTEM_USER_MANUAL.md](./AI_SYSTEM_USER_MANUAL.md)
- [PandasAnalysisEngine ì½”ë“œ](./ksys_app/ai_engine/pandas_analysis_engine.py)
- [RealDataAuditSystem ì½”ë“œ](./ksys_app/ai_engine/real_data_audit_system.py)

### í•™ìŠµ ë¦¬ì†ŒìŠ¤
- **ê°•í™”í•™ìŠµ**: Sutton & Barto, "Reinforcement Learning: An Introduction"
- **Multi-Agent Systems**: Wooldridge, "An Introduction to MultiAgent Systems"
- **RAG Systems**: Lewis et al., "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
- **Continuous Learning**: Chen & Liu, "Lifelong Machine Learning"

---

## ğŸ“ ì§€ì› ë° ë¬¸ì˜

**ê¸°ìˆ  ì§€ì›**: AI Engine ê°œë°œíŒ€  
**ë¬¸ì„œ ë²„ì „**: 1.0  
**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-09-01

---

*ì´ ë¬¸ì„œëŠ” KSys RAG ì‹œìŠ¤í…œì˜ ì§€ì†ì ì¸ ë°œì „ì„ ìœ„í•œ í¬ê´„ì ì¸ í•™ìŠµ ì „ëµì„ ì œì‹œí•©ë‹ˆë‹¤. ì‹¤ì œ êµ¬í˜„ ì‹œì—ëŠ” ì‹œìŠ¤í…œ íŠ¹ì„±ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ì¡°ì •í•˜ì—¬ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.*

